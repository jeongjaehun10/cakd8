{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfe459-f22a-4eb4-929d-6fa6cb5d7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "활성화 함수란? 활성화 함수(Activation function)이란 입력된 데이터의 가중\n",
    "합을 출력 신호로 변환하는 함수\n",
    "\n",
    "활성화 함수의 종류는? 시그모이드 함수, Tanh, 소프트 맥스 ,Relu함수\n",
    "\n",
    "손실함수 종류 : MSE, MAE, Categorical Crossentropy(2개이상), Binary Crossentropy\n",
    "\n",
    "옵티마이저란? 옵티마이저는 가장 효율적인 방법으로 SQL을 수행할 \n",
    "최적의 처리 경로를 생성해주는 DBMS의 핵심 엔진입니다. 최적화된 파라미터 \n",
    "\n",
    "옵티마이저의 종류는? RMSPROP\n",
    "경사 하강법(Gradient Descent)\n",
    "확률적 경사 하강법(Stochastic Gradient Descent, SGD\n",
    "\n",
    "소프트맥스 함수는 다중 클래스 분류 모델을 만들 때 사용한다. 결과를 확률로 해석할 수 있게 변환해주는 함수로 높은\n",
    "확률을 가지는 class로 분류한다. 이는 결과값을 정규화시키는 것으로도 생각할 수 있다. \n",
    "K는 클래스 수를 나타내며, zj\n",
    "는 소프트맥스 함수의 입력값이다. pj를 직관적으로 해석하면 j번째입력값/입력값의합\n",
    "\n",
    " softmax함수에서 K=2로 두면 sigmoid함수로 환원이 되고, 반\n",
    "대로 sigmoid함수를 K개의 클래스로 일반화하면 softmax함수가\n",
    "유도된다. \n",
    " sigmoid는 활성화 함수로 softmax는 출력층에 사용되지만, 수학\n",
    "적으로는 서로 같은 함수이다.           \n",
    "이진 분류 문제에 sigmoid 함수 대신 softmax 함수를 쓸 수도 있다.\n",
    "이진 분류 문제에서는 두 함수가 사실상 같은 역할을 한다.\n",
    "범주가 여러 개일 때 sigmoid 함수를 쓰면 출력값의 합이 1이 아니지\n",
    "만, softmax 함수를 쓰면 정의에서 출력값의 합이 항상 1이 된다.\n",
    "(물론 활성화 함수를 선택하는 것은 자유이며, 다른 함수를 써도 된다.)\n",
    "           \n",
    "           \n",
    "⑴ 지수함수의 정의\n",
    "\n",
    "① 일 때, 실수 에 의 값을 대응시키면 각각의 에 대하여 의 값이 단 하나로 정해지므로 는 의 함수\n",
    "           \n",
    "           \n",
    "으로 볼 수 있으며 따라서 확률 관점으로 볼 수 있다. 지수함수가 사용되는 이유는 미분이 가능하도록 하게 함이며, \n",
    "입력값 중 큰 값은 더 크게 작은 값은 더 작게 만들어 입력벡터가 더 잘 구분되게 하기 위함이다. \n",
    "차원 데이터 벡터를 소프트맥스 함수의 입력 벡터로 차원을 축소하는 방법은 간단하다. \n",
    "소프트맥스 함수의 입력 벡터 z의 차원수만큼 결과값이 나오도록 가중치 곱을 진행한다.\n",
    "위의 그림에서 화살표는 총 (4 x 3 = 12) 12개이며 전부 다른 가중치를 가지고,\n",
    "학습 과정에서 점차적으로 오차를 최소화하는 가중치로 값이 업데이트됩니다. \n",
    "데이터의 실제값이 setosa라면, setosa의 원-핫 벡터는 [0 1 0]이다. 이 경우, 예측값과 실제값의 오차가 0이 되는 경우는 \n",
    "소프트맥스 함수의 결과가 [0 1 0]이 되는 것이다.\n",
    "이 두 벡터 [0.26 0.70 0.04] [0 1 0] 의 오차를 계산하기 위해서 소프트맥스 회귀는 손실함수로 cross-entropy 함수를 사용한다. \n",
    "Softmax(소프트맥스)는 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며\n",
    "출력 값들의 총합은 항상 1이 되는 특성을 가진 함수이다. \n",
    "공식1에서 분자에 있는 지수함수가 각각의 값의 편차를 확대시키는 역할을 한다. \n",
    "지수함수 그래프를 생각해보면 입력값이 커질수록 기하급수적으로 출력값이 커짐을 알 수 있다\n",
    "따라서 K개 값 중에서 큰 값이었던 것은 상대적으로 확실히 커지게 되고, 작은 값이었던 것은 상대적으로 작아지게 된다.\n",
    "분모에 모든 K값의 지수함수값을 모두 더했기 때문에 상대성을 갖게 되는 것이다. \n",
    "\n",
    " \n",
    "\n",
    "그리고 또 하나 중요한 것은 softmax 함수를 거친 K개 값의 합은 1이 된다는 것이다.             \n",
    "           \n",
    "           \n",
    "뉴런의 출력값에 대하여 class 분류를 위하여 마지막 단계에서 출력값에 대한 정규화를 해주는 함수이다.\n",
    "\n",
    "인물 사진을 예로 들어 보겠다.\n",
    "\n",
    "사진속 인물이 지금 슬픈 표정인지, 웃는 표정인지, 화난 표정인지 확률적으로 수치화한다고 했을때,\n",
    "\n",
    "슬픔 (11%), 웃음 (29%), 화남(60%) 화같이 확률적 classification 을 할 때 용이하다.\n",
    "\n",
    "소프트맥스 함수의 특징은 결과물의 수치의 합은 언제나 1.0 이다.\n",
    "           \n",
    "           \n",
    "Optimizer 종류\n",
    " 오차에 대하여 w를 업데이트 시키는 알고리즘들.\n",
    " GD(Gradient Descent)\n",
    " Batch GD, Mini-Batch GD\n",
    " SGD(Stochastic GD)\n",
    " Momentum\n",
    " AdaGrad, AdaDelta\n",
    " Adam\n",
    " RMSprop\n",
    "\n",
    "           \n",
    "그런데 2개 이상의 변수부터는 머리가 좀 아프죠?\n",
    "그렇기 때문에 변수 하나 빼고는 모두 고정하고,\n",
    "즉 나머지를 모두 상수로 취급해준 상태로\n",
    "미분을 취하는 것이 바로 편미분이지요!\n",
    "           \n",
    "           \n",
    "           \n",
    "           Gradient Descent\n",
    " 함수가 학습될 바를 정의한 비용함수의 값이 최소로 하기 위해\n",
    "가중치를 업데이트 하기 위한 알고리즘\n",
    "경사하강법은 비용함수를 구하고, 이를 각 weight 별로 편미분하면 각 weight 별로 수정할\n",
    "다음 값을 구할 수 있다. 그렇다고 한다.\n",
    "선형 모델(w1x1 + w2x2 = b)에서는 경사하강법의 결과가 직관적으로 설명했던 방법과\n",
    "동일하다.\n",
    "결국 사과예에서 학습한 방법은 경사하강법을 사용한 것이다\n",
    "\n",
    "경사 하강법(Gradient Descent)이란 딥러닝 알고리즘 학습 시 사용되는 최적화 방법 중 하나입니다. \n",
    "딥러닝 알고리즘 학습 시 목표는 예측값과 정답값 간의 차이인 손실 함수의 크기를 최소화시키는 파라미터를 찾는 것입니다.\n",
    "학습 데이터 입력을 변경할 수 없기 때문에, 손실 함수 값의 변화에 따라 가중치(weight) 혹은 편향(bias)을 업데이트해야 합니다.\n",
    "\n",
    "global minimum은 함수 전체에서 가장 낮은 곳을 말하며 최솟값이라고 합니다.\n",
    "\n",
    "local minimum은 함수에서 부분적으로 낮은 곳을 말하며 극소값이라고 합니다.\n",
    "\n",
    "경사 하강법(Gradient Descent)은 머신러닝 및 딥러닝 알고리즘을 학습시킬때 사용하는 방법 중 하나이며 \n",
    "1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기(경사)를\n",
    "구하여 기울기가 낮은 쪽으로 계속 이동시켜 극값(최적값)에 이를 때까지 반복하는 것이다.\n",
    "경사 하강법에서는 학습시 스텝의 크기 (step size)가 중요하다. 학습률이 너무 작을 경우 알고리즘이 수렴하기 위해 반복해야 하는 \n",
    "값이 많으므로 학습 시간이 오래걸린다. 그리고 지역 최소값(local minimum)에 수렴할 수 있다. 반대로 학습률이 너무 클 경우 \n",
    "학습 시간은 적게 걸리나, 스텝이 너무 커서 전역 최소값(global minimum)을 가로질러 반대편으로 \n",
    "건너뛰어 최소값에서 멀어질 수 있다.\n",
    "\n",
    "           \n",
    "경사 하강법은 전체 데이터를 모두 사용해서 기울기를 계산(Batch Gradient Descent)하기 때문에 학습하는데 많은 시간이 필요하다.\n",
    "만약 10만개의 데이터가 있을때, 데이터에 업데이트가 있을 때 마다 10만번의 계산을 해야한다. \n",
    "그래서 학습 데이터가 큰 경우 부담이 있다. 이러한 느린점을 보완하기 위해서 \n",
    "확률적 경사 하강법(Stochastic Gradient Descent)을 사용한다. \n",
    "이 방법은 매 step에서 딱 한개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 기울기를 계산한다. \n",
    "확률적 경사 하강법은 아래와 같은 특징이 있다.\n",
    "매우 적은 데이터를 처리하기 때문에 학습 속도가 빠름\n",
    "하나의 샘플만 메모리에 있으면 되기 대문에 큰 데이터셋도 학습이 가능\n",
    "Cost function이 매우 불규칙할 경우 알고리즘이 local minimum을 건너뛰도록 도와주어 global minimum을 찾을 가능성이 높음\n",
    "\n",
    "           \n",
    "경사하강법 알고리즘\n",
    " SGD(Stochastic GD)\n",
    " Momentum\n",
    " AdaGrad, AdaDelta\n",
    " RMSprop\n",
    "           \n",
    "BackPropagation 오차 역 전파\n",
    " 출력된 값과 원하는 값과의 차이를 가지고 그 전의 w 값들을\n",
    "변경하는 알고리즘.\n",
    " 뒤에서부터 그 오차의 값이 전파된다는 이름.\n",
    " 실제 변경되는 값의 크기는 GD로 결정됨\n",
    " 경사하강법 + 연쇄법칙\n",
    "\n",
    "학습율\n",
    " 가중치가 변경되는 정도\n",
    "           \n",
    "           \n",
    "Overfitting 방지법 종류\n",
    " DropOut\n",
    " Batch Normalization\n",
    " Regularization (L1, L2 penalty on weights)\n",
    " Early stopping\n",
    " Data Augmentation\n",
    "\n",
    "오버피팅 방지\n",
    " Batch Normalization이나 DropOut도 효과가 좋다.\n",
    "\n",
    "Drop out이란?\n",
    "p 확률로 선택된것만 확습을 시킴 (확률적으로 선택하기 때문에 랜덤)\n",
    "노드에 연결된 엣지가 없는 것으로 간주  \n",
    "\n",
    " 하지만 핵심은 충분한 데이타양이다.\n",
    " 데이타 증강(data augmentation)을 추천\n",
    " 이미지의 경우\n",
    "- 회전\n",
    "- 반전\n",
    "- 축소, 확대\n",
    "- 밝기, 명암대비 확대/축소\n",
    "- 색상 변경\n",
    "           \n",
    "           \n",
    "1. 머신러닝/딥러닝의 목적은 일반화이다.\n",
    "\n",
    "2. 일반화란, 특정 시점의 특정 현상을 분석하여 얻은 식을 새로운 시점의 새로운 현상에 적용하는 것을 말한다.\n",
    "\n",
    "​\n",
    "\n",
    "3. 보간이란, 데이터 샘플 사이의 공백(NULL)을 전후의 데이터로 추정하여 채워넣는 것을 의미한다.\n",
    "\n",
    "4. 보간의 원리는 매니폴드 가설을 따른다.\n",
    "\n",
    "​\n",
    "\n",
    "5. 매니폴드 가설이란?\n",
    "\n",
    "  1) 매니폴드 : 한 공간이 부분적으로 선형(유클리드) 공간과 유사할 때, 그 안에 형성되는 저차원 부분 공간\n",
    "\n",
    "  2) 매니폴드 가설 :\n",
    "\n",
    "    ① 전제 : 실제 세상의 모든 데이터는 고차원 공간 안에 있는 저차원 매니폴드에 속한다고 가정. 즉, 모든 데이터는 어떤 고차원 공간 안에서 선형으로 표현되는 일부에 대한 저차원 부분 공간이다.\n",
    "\n",
    "    ② 가설\n",
    "\n",
    "      -. 어떤 현상을 이해하기 위한 머신러닝 모델은, 가능한 입력 공간 안에서 간단하고 저차원이며 매우 구조적인 부분 공간만 학습하면 된다.\n",
    "\n",
    "      -. 선형의 부분공간이기 때문에 두 입력 사이를 보간하는 것이 항상 가능하다. 이는 즉, 공간 안의 샘플만 이용해서 공간 전체를 이해할 수도 있다는 뜻이다.\n",
    "\n",
    "​\n",
    "\n",
    "6. 지역 일반화 : 그러나 보간이 만능통치약은 아니다. 오직 이전에 본 데이터와 매우 가까운 것을 이해하는데에만 도움을 줄 뿐이다. 이를 지역 일반화라고 한다.\n",
    "\n",
    "​\n",
    "\n",
    "7. 결국 중요한 것은 데이터\n",
    "\n",
    "  1) 특성공학과 데이터 큐레이션 : 일반화는 데이터의 자연적 구조에 의한 자연스런 속성이며, 오직 데이터가 보간할 수 있는 매니폴드를 형성하는 경우에만 일반화가 가능함. 이를 위해 특성이 유익하고 잡음이 적어먀하므로 데이터 큐레이션과 특성공학이 모델링에 굉장히 중요함을 알아야함.\n",
    "\n",
    "  2) 조밀한 샘플링 : 입력 데이터가 매니폴드 전체를 조밀하게 커버해야함. 즉, 편향성을 띄지 않은 채로 모집단의 분포를 따라야한다는 뜻.\n",
    "\n",
    "  3) 데이터는 좋을수록(잡음이 없을수록) 좋고, 많을수록 좋다.\n",
    "\n",
    "  4) 딥러닝 모델은 보간을 통해 일반화를 돕는 것에 그칠 뿐이므로, 엔지니어는 모델이 쉽게 보간할 수 있도록 최선을 다해야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f667e83-8eb6-4917-950c-80ed57635d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "000xxx\n",
    "000xxx\n",
    "000xxx\n",
    "\n",
    "000000\n",
    "000000\n",
    "000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a999cd3-437c-48e4-8df7-2059f4a51cde",
   "metadata": {},
   "source": [
    "# 기존 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872a9bb5-727b-4797-afb6-4ad8014a4782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2566 - acc: 0.9256\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1036 - acc: 0.9689\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0684 - acc: 0.9796\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0496 - acc: 0.9845\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0378 - acc: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ac5ec3afd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential # 대문자는 class다.\n",
    "\n",
    "(train_images, train_labels),(test_images, test_labels)=mnist.load_data()\n",
    "# 딥러닝은 소수로 하는게 훨씬 부담이 덜 함 0과 1사이 숫자로 바뀜\n",
    "train_images=train_images.reshape(60000, 28*28) # 3차원 자료를 한줄로 세움 \n",
    "train_images=train_images/255.0 #각각의 숫자의 음영이 255까지 있음\n",
    "\n",
    "test_images=test_images.reshape(10000, 28*28) \n",
    "test_images=test_images/255.0     \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 512개의 층을 통과하여 600000, 784개의 속성이 10게로 바꿔져서 나온다.\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics='acc')\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c8d809-17b2-4ed0-8a9b-bc31c4d2a9e5",
   "metadata": {},
   "source": [
    "# 소음 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346787a2-2401-4fd1-99c1-a3dd5120c1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.6187 - acc: 0.8132 - val_loss: 0.2660 - val_acc: 0.9199\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.2490 - acc: 0.9234 - val_loss: 0.1793 - val_acc: 0.9457\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.1607 - acc: 0.9505 - val_loss: 0.1742 - val_acc: 0.9464\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.1122 - acc: 0.9646 - val_loss: 0.1269 - val_acc: 0.9603\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0843 - acc: 0.9728 - val_loss: 0.1212 - val_acc: 0.9659\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0589 - acc: 0.9816 - val_loss: 0.1357 - val_acc: 0.9613\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0445 - acc: 0.9857 - val_loss: 0.1349 - val_acc: 0.9631\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0346 - acc: 0.9889 - val_loss: 0.1375 - val_acc: 0.9655\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0260 - acc: 0.9920 - val_loss: 0.1620 - val_acc: 0.9612\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0198 - acc: 0.9935 - val_loss: 0.1379 - val_acc: 0.9686\n",
      "Epoch 1/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0490 - acc: 0.9848 - val_loss: 0.0895 - val_acc: 0.9768\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0327 - acc: 0.9897 - val_loss: 0.0894 - val_acc: 0.9770\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0245 - acc: 0.9928 - val_loss: 0.0879 - val_acc: 0.9778\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0192 - acc: 0.9943 - val_loss: 0.0868 - val_acc: 0.9783\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0146 - acc: 0.9958 - val_loss: 0.0867 - val_acc: 0.9788\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0112 - acc: 0.9969 - val_loss: 0.0888 - val_acc: 0.9801\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0084 - acc: 0.9980 - val_loss: 0.0946 - val_acc: 0.9768\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0932 - val_acc: 0.9788\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.0981 - val_acc: 0.9803\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.1012 - val_acc: 0.9797\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential # 대문자는 class다.\n",
    "import numpy as np\n",
    "\n",
    "(train_images, train_labels),(test_images, test_labels)=mnist.load_data()\n",
    "# 딥러닝은 소수로 하는게 훨씬 부담이 덜 함 0과 1사이 숫자로 바뀜\n",
    "train_images=train_images.reshape(60000, 28*28) # 3차원 자료를 한줄로 세움 \n",
    "train_images=train_images/255.0 #각각의 숫자의 음영이 255까지 있음\n",
    "\n",
    "train_images_with_noise_channels =np.concatenate([train_images, np.random.random((len(train_images), 784))], axis=1)\n",
    "train_images_with_zeros_channels =np.concatenate([train_images, np.zeros((len(train_images), 784))], axis=1)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 512개의 층을 통과하여 600000, 784개의 속성이 10게로 바꿔져서 나온다.\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics='acc')\n",
    "history_noise=model.fit(train_images_with_noise_channels, train_labels, epochs=10, batch_size=128, validation_split=0.2) \n",
    "history_zeros=model.fit(train_images_with_zeros_channels, train_labels, epochs=10, batch_size=128, validation_split=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1546eab4-4c51-44c4-99f9-cc854a0c516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ac196e6a90>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_acc_noise = history_noise.history[\"val_acc\"]\n",
    "val_acc_zeros = history_zeros.history[\"val_acc\"]\n",
    "epochs= range(1,11)\n",
    "plt.plot(epochs, val_acc_noise, \"b-\", label=\"noise\")\n",
    "plt.plot(epochs, val_acc_zeros, \"b-\", label=\"zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2f05f-7a72-4a49-8239-b82212d70883",
   "metadata": {},
   "outputs": [],
   "source": [
    "기계가 스스로 처리 ai\n",
    "신경망 di\n",
    "Bx + B(편향, 자유로운 원점에 고정되지 않음) =y 단순선형회귀 \n",
    "단순선형회귀를 여러개 신경망이다.\n",
    "dnn 함수근사회 능력 \n",
    "미분= 도함수 =기울기 =(미분계수)= Gradient = 그래디언트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee043e-1e20-49ef-9d88-38432b44ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential \n",
    "import numpy as np \n",
    "\n",
    "(train_images, train_labels), _ = mnist.load_data() # 언패킹.. _로 안받을 수도 있음\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(0.01), # 학습률을 1.0으로 잡음\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,  \n",
    "          \n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2) \n",
    "\n",
    "\n",
    "(train_images, train_labels), _ = mnist.load_data() # 언패킹.. _로 안받을 수도 있음\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1.), # 학습률을 1.0으로 잡음\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79766bd8-1a97-4251-b972-fa68111b8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data() # 언패킹.. _로 안받을 수도 있음\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(0.01), # 학습률을 1.0으로 잡음\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae8f92-1a65-49fd-a881-96f9ac0e479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data() # 언패킹.. _로 안받을 수도 있음\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(0.01), # 학습률을 1.0으로 잡음\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
