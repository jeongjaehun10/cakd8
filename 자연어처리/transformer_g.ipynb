{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODrXieP2C9hZKZZOAC2YTq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EW-0Asqy4Q0l","executionInfo":{"status":"ok","timestamp":1673224727799,"user_tz":-540,"elapsed":18296,"user":{"displayName":"정재훈","userId":"01263813800591133648"}},"outputId":"bbe86ed0-0f36-464e-f48a-a50b21606962"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"markdown","source":["# 트랜스포머 아키텍처"],"metadata":{"id":"IVpP7SZ14GJb"}},{"cell_type":"code","source":["# 셀프 어텐션 - 각각의 단어들에 대해서 attention하는 것이다. 밥과 관계되는 나는 먹었다 다본다. 단어의 수만큼 발생된다. 충분한 데이터 기반 학습이 가능하다. 전체적인 문맥을 아니까한다. \n","# 두번째에는 i가 세번째에는 i ate이 들어가고,\n","# transformer가 인코더 디코더로 간다. multi attention은 self attention이라고 한다.\n","# attention은 인코더 디코더 다 봐서 한다.\n","# self attention은 인코더나 디코더나 본다. 만약 인코더만 본다면 단어 중심으로 관계를 다 따져본다. \n","# 어텐션은 (어제, 카페, 많더라 다 따져본다. 셀프 어텐션은 입력시퀀싀 각각의 단어들 다따짐)\n","# 셀프어텐션은 rrn없이 셀프어텐션은 인코더 , 디코더 각각의 단어들 수만큼 작동한다. "],"metadata":{"id":"AcBn26Gt4TZ2","executionInfo":{"status":"ok","timestamp":1673223695904,"user_tz":-540,"elapsed":2,"user":{"displayName":"정재훈","userId":"01263813800591133648"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### 셀프 어텐션 이해하기\n","\n","- 트랜스포머 구조에서 멀티 헤드 어텐션은 셀프 어텐션(self attention)이라고도 불립니다. \n","- 트랜스포머 경쟁력의 원천은 셀프 어텐션에 있다.\n","- 어텐션(attention)은 시퀀스 입력에 수행하는 기계학습 방법의 일종으로 시퀀스 요소들 가운데 태스크 수행에 중요한 요소에 집중하고 그렇지 않은 요소는 무시해 태스크 수행 성능을 끌어 올리며 기계 번역 과제에 처음 도입되었다.\n","- 기계 번역에 어텐션을 도입한다면 타깃 언어를 디코딩할 때 소스 언어의 단어 시퀀스 가운데 디코딩에 도움되는 단어들 위주로 취사 선택해서 번역 품질을 끌어 올리게 된다. 즉, 어텐션은 디코딩할 때 소스 시퀀스 가운데 중요한 요소들만 추린다.\n","- 셀프 어텐션이란, 말 그대로 자기 자신에 수행하는 어텐션 기법으로 입력 시퀀스 가운데 태스크 수행에 의미 있는 요소들 위주로 정보를 추출한다는 것이다.\n","- 합성곱 신경망과 비교 : CNN은 합성곱 필터(convoltion filter) 라는 특수한 장치를 이용해 시퀀스의 지역적인 특징을 잡아내는 모델이다. 자연어는 기본적으로 시퀀스(단어 혹은 형태소의 나열)이고 특정 단어 기준 주변 문맥이 의미 형성에 중요한 역할을 하므로 CNN이 자연어 처리에 널리 쓰인다.하지만 CNN은 합성곱 필터 크기를 넘어서는 문맥은 읽어내기 어렵다는 단점이 있습니다. 예컨대 필터 크기가 3(3개 단어씩 처리)이라면 4칸 이상 떨어져 있는 단어 사이의 의미는 캐치하기 어렵다.\n","- 순환 신경망과 비교 : RNN 역시 시퀀스 정보를 압축하는 데 강점이 있는 구조이다. 소스 언어 시퀀스인 어제, 카페, 갔었어, 거기, 사람, 많더라를 인코딩해야 한다고 가정해 보면 RNN은 소스 시퀀스를 차례대로 처리한다. 하지만 RNN은 시퀀스 길이가 길어질 수록 정보 압축에 문제가 발생하며 오래 전에 입력된 단어는 잊어버리거나, 특정 단어 정보를 과도하게 반영해 전체 정보를 왜곡하는 경우가 자주 생긴다.\n","기계 번역을 할 때 RNN을 사용한다면 인코더가 디코더로 넘기는 정보는 소스 시퀀스의 마지막인 많더라라는 단어의 의미가 많이 반영될 수밖에 없으며 RNN은 입력 정보를 차례대로 처리하고 오래 전에 읽었던 단어는 잊어버리는 경향이 있다.\n","- 어텐션과 비교 : cafe에 대응하는 소스 언어의 단어는 카페이고 이는 소스 시퀀스의 초반부에 등장한 상황에서 cafe라는 단어를 디코딩해야 할 때 카페를 반드시 참조해야 한다. 어텐션이 없는 단순 RNN을 사용하면 워낙 초반에 입력된 단어라 모델이 잊었을 가능성이 크고, 이 때문에 번역 품질이 낮아질 수 있다. 어텐션은 이러한 문제점을 해결하기 위해 제안되었으며 디코더 쪽 RNN에 어텐션을 추가하는 방식이다. 어텐션은 디코더가 타깃 시퀀스를 생성할 때 소스 시퀀스 전체에서 어떤 요소에 주목해야 할지 알려주므로 카페가 소스 시퀀스 초반에 등장하거나 소스 시퀀스의 길이가 길어지더라도 번역 품질이 떨어지는 것을 막을 수 있다.\n","- 셀프 어텐션은 자기 자신에 수행하는 어텐션이다.입력 시퀀스가 어제, 카페, 갔었어, 거기, 사람, 많더라일 때 거기라는 단어가 어떤 의미를 가지는지 계산하는 상황에서 잘 학습된 셀프 어텐션 모델이라면 거기에 대응하는 장소는 카페라는 사실을 알아챌 수 있다. 그뿐만 아니라 거기는 갔었어와도 연관이 있음을 확인할 수 있다. 트랜스포머 인코더 블록 내부에서는 이처럼 거기라는 단어를 인코딩할 때 카페, 갔었어라는 단어의 의미를 강조해서 반영한다.\n","- 셀프 어텐션 수행 대상은 입력 시퀀스 전체이며 개별 단어와 전체 입력 시퀀스를 대상으로 어텐션 계산을 수행해 문맥 전체를 고려하기 때문에 지역적인 문맥만 보는 CNN 대비 강점이 있다. 아울러 모든 경우의 수를 고려(단어들 서로가 서로를 1대 1로 바라보게 함)하기 때문에 시퀀스 길이가 길어지더라도 정보를 잊거나 왜곡할 염려가 없다. 이는 RNN의 단점을 극복한 지점입이다.\n","\n","어텐션과 셀프 어텐션의 주요 차이\n","- 어텐션은 소스 시퀀스 전체 단어들(어제, 카페, …, 많더라)과 타깃 시퀀스 단어 하나(cafe) 사이를 연결하는 데 쓰인다. 반면 셀프 어텐션은 입력 시퀀스 전체 단어들(그림15, 그림16) 사이를 연결한다.\n","- 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작한다.\n","타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만 셀프어텐션은 인코더, 디코더 블록의 개수만큼 반복 수행한다.\n","#### 일반화된 셀프 어텐션: 쿼리-키-값 모델\n","\n","- 셀프 어텐션은 쿼리(query), 키(key), 밸류(value) 세 가지 요소가 서로 영향을 주고 받는 구조이다. 트랜스포머 블록에는 문장 내 각 단어가 벡터(vector) 형태로 입력되는데 여기서 벡터란 숫자의 나열 정도로 이해할 수 있다.\n","- 각 단어 벡터는 블록 내에서 어떤 계산 과정을 거쳐 쿼리, 키, 밸류 세 가지로 변환된다. 만일 트랜스포머 블록에 입력되는 문장이 여섯 개 단어로 구성돼 있다면 이 블록의 셀프 어텐션 계산 대상은 쿼리 벡터 6개, 키 벡터 6개, 밸류 백터 6개 등 모두 18개가 된다. 셀프 어텐션은 쿼리 단어 각각에 대해 모든 키 단어와 얼마나 유기적인 관계를 맺고 있는지 그 합이 1인 확률값으로 나타낸다. 카페라는 쿼리 단어와 가장 관련이 높은 키 단어는 거기라는 점(0.4)을 확인할 수 있다.\n","- 셀프 어텐션 모듈은 밸류 벡터들을 가중합(weighted sum)하는 방식으로 계산을 마무리한다. 새롭게 만들어지는 카페 벡터( Z카페 )는 문장에 속한 모든 단어 쌍 사이의 관계가 녹아 있다.\n","Z카페=0.1×V어제+0.1×V카페+0.1×V갔었어+0.4×V거기+0.2×V사람+0.1×V많더라\n","- 카페에 대해서만 계산 예를 들었지만 이러한 방식으로 나머지 단어들도 셀프 어텐션을 각각 수행한다. 모드 시퀀스를 대상으로 셀프 어텐션 계산이 끝나면 그 결과를 다음 블록으로 넘깁니다. 이처럼 트랜스포머 모델은 셀프 어텐션을 블록(레이어) 수만큼 반복합니다.\n","\n"],"metadata":{"id":"e9L20wMx52nF"}},{"cell_type":"markdown","source":["셀프 어텐션 동작 원리\n","https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/"],"metadata":{"id":"XppyQrHd53d5"}},{"cell_type":"markdown","source":["셀프 어텐션 출력 과정 : 쿼리, 키, 밸류 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n","- 입력(단어 개수, 단어 임베딩 차원 수)\n","- 쿼리, 키, 밸류 만들기 : 세가지 행렬은 태스크를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트\n","- 셀프 어텐션 계산\n","  - 쿼리 벡터들을 한꺼번에 모아서 키 벡터들과 행렬곱을 수행\n","  - 소프트맥스 확률값 만들기 : 키 벡터의 차원수의 제곱근으로 나눠준 뒤 소프트맥스를 취하는 과정\n","  - 소프트맥스 확률과 밸류를 가중합하기\n"],"metadata":{"id":"jf-bLNl091a5"}},{"cell_type":"code","source":["import torch\n","# 입력 벡타 시퀀스 (단어 개수 3, 차원수 4)\n","x = torch.tensor([\n","  [1.0, 0.0, 1.0, 0.0],\n","  [0.0, 2.0, 0.0, 2.0],\n","  [1.0, 1.0, 1.0, 1.0],  \n","])\n","\n","# 셀프 어텐션은 쿼리 , 키 벨류 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n","# 쿼리, 키 , 벨류를 만들어 주는 행렬(w)\n","w_query = torch.tensor([\n","  [1.0, 0.0, 1.0],\n","  [1.0, 0.0, 0.0],\n","  [0.0, 0.0, 1.0],\n","  [0.0, 1.0, 1.0]\n","])\n","w_key = torch.tensor([\n","  [0.0, 0.0, 1.0],\n","  [1.0, 1.0, 0.0],\n","  [0.0, 1.0, 0.0],\n","  [1.0, 1.0, 0.0]\n","])\n","w_value = torch.tensor([\n","  [0.0, 2.0, 0.0],\n","  [0.0, 3.0, 0.0],\n","  [1.0, 0.0, 3.0],\n","  [1.0, 1.0, 0.0]\n","])"],"metadata":{"id":"Bgd4R2u0-WWG","executionInfo":{"status":"ok","timestamp":1673224881102,"user_tz":-540,"elapsed":3574,"user":{"displayName":"정재훈","userId":"01263813800591133648"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 쿼리, 키, 밸류 만들기\n","# w_key, w_query, w_value 세가지 행렬은 데스크를 가장 잘 수행할 수 있는 방향을 학습과정에서 업데이트\n","keys = torch.matmul(x, w_key) #입력값과 key의 가중치를 곱해서 key 값을 만든다 아래도 같다.\n","querys = torch.matmul(x, w_query)\n","values = torch.matmul(x, w_value)\n","print(keys, '\\n')\n","print(querys, '\\n')\n","print(values, '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4vkwHln-XG7","executionInfo":{"status":"ok","timestamp":1673225413923,"user_tz":-540,"elapsed":8,"user":{"displayName":"정재훈","userId":"01263813800591133648"}},"outputId":"52174e73-32a2-4711-b73a-ddc5f13ba886"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 1.],\n","        [4., 4., 0.],\n","        [2., 3., 1.]]) \n","\n","tensor([[1., 0., 2.],\n","        [2., 2., 2.],\n","        [2., 1., 3.]]) \n","\n","tensor([[1., 2., 3.],\n","        [2., 8., 0.],\n","        [2., 6., 3.]]) \n","\n"]}]},{"cell_type":"code","source":["# 쿼리 벡터들을 한꺼번에 모아서 키 벡터들과 행렬곱 수행\n","attn_scores = torch.matmul(querys, keys.T)\n","print(querys, '\\n')\n","print(keys.T, '\\n')\n","attn_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WczaeRxW_AYj","executionInfo":{"status":"ok","timestamp":1673225472323,"user_tz":-540,"elapsed":371,"user":{"displayName":"정재훈","userId":"01263813800591133648"}},"outputId":"12c6c551-810c-48e7-ba31-3775e586da26"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 2.],\n","        [2., 2., 2.],\n","        [2., 1., 3.]]) \n","\n","tensor([[0., 4., 2.],\n","        [1., 4., 3.],\n","        [1., 0., 1.]]) \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2.,  4.,  4.],\n","        [ 4., 16., 12.],\n","        [ 4., 12., 10.]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["print(keys, '\\n')\n","print(keys.shape[-1], '\\n')\n","np.sqrt(keys.shape[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYCHVxa1D_ig","executionInfo":{"status":"ok","timestamp":1673226486765,"user_tz":-540,"elapsed":3,"user":{"displayName":"정재훈","userId":"01263813800591133648"}},"outputId":"52d8656a-3bb9-4c64-d49e-2e0e45fe69e9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 1.],\n","        [4., 4., 0.],\n","        [2., 3., 1.]]) \n","\n","3 \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["1.7320508075688772"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# 소프트 맥스 확률값 만들기 : 키 벡터의 차원수의 제곱근으로 나눠준 뒤 소프트 맥스를 취하는 과정\n","import numpy as np\n","from torch.nn.functional import softmax\n","key_dim_sqrt = np.sqrt(keys.shape[-1]) # 차원은 열이다. \n","attn_probs = softmax(attn_scores / key_dim_sqrt, dim=-1)\n","attn_probs "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wXjwYsR_5ci","executionInfo":{"status":"ok","timestamp":1673226580478,"user_tz":-540,"elapsed":339,"user":{"displayName":"정재훈","userId":"01263813800591133648"}},"outputId":"486ba3fd-9b1c-43d8-af1a-9d56b093fa40"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n","        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n","        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# 소프트맥스 확률과 벨류를 가중합하기\n","weighted_values = torch.matmul(attn_probs, values)\n","print(values, '\\n')\n","weighted_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5u140jy_8Ez","executionInfo":{"status":"ok","timestamp":1673226614761,"user_tz":-540,"elapsed":5,"user":{"displayName":"정재훈","userId":"01263813800591133648"}},"outputId":"6f8ec1ea-223c-4356-b944-a3d9a1b4b403"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2., 3.],\n","        [2., 8., 0.],\n","        [2., 6., 3.]]) \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1.8639, 6.3194, 1.7042],\n","        [1.9991, 7.8141, 0.2735],\n","        [1.9926, 7.4796, 0.7359]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["트랜스포머에 적용된 기술\n","- 인코더와 디코더 블록의 구조는 멀티 헤드 어텐션, 피드포워드 뉴럴 네트워크, 잔차 연결 및 레이어 정규화 등 세 가지 구성 요소를 기본\n","- 피드포워드 뉴럴네트워크\n","  - 입력은 현재 블록의 멀티 헤드 어텐션의의 개별 출력 벡터\n","  - 피드포워드 뉴럴네트워크란 신경망(neural network)의 한 종류로 그림2와 같이 입력층(input layer,  x ), 은닉층(hidden layer,  h ), 출력층(ouput layer,  y ) 3개 계층으로 구성\n","  - 이전 뉴런 값과 그에 해당하는 가중치를 가중합(weighted sum)한 결과에 바이어스(bias)를 더해 만듭니다. 가중치들과 바이어스는 학습 과정에서 업데이트. 활성 함수(activation function,  f )는 현재 계산하고 있는 뉴런의 출력을 일정 범위로 제한하는 역할. 활성함수는 ReLU(Rectified Linear Unit)\n","  - 입력층 뉴런이 각각  [2,1] 이고 그에 해당하는 가중치가  [3,2] , 바이어스(bias)가 1이라고 가정해 보겠습니다. 그러면 은닉층 첫번째 뉴런 값은  2×3+1×2+1=9 가 되며 이 값은 양수이므로 ReLU를 통과해도 그대로 유지\n","  - 트랜스포머에서는 은닉층의 뉴런 갯수(즉 은닉층의 차원수)를 입력층의 네 배로 설정. 예컨대 피드포워드 뉴럴네트워크의 입력 벡터가 768차원일 경우 은닉층은 2048차원까지 늘렸다가 출력층에서 이를 다시 768차원으로 줄인다.\n","- 잔차 연결\n","  - 잔차 연결이란 블록(block) 계산을 건너뛰는 경로를 하나 두는 것을 의미\n","  - 입력을  x , 이번 계산 대상 블록을  F 라고 할 때 잔차 연결은  F(x)+x 로 간단히 실현\n","  원래 fx인데 건너뛰어서 +x함 \n","  - 잔차 연결을 두지 않았을 때는  f1 ,  f2 ,  f3 을 연속으로 수행하는 경로 한 가지만 존재하였으나, 잔차 연결을 블록마다 설정해둠으로써 모두 8가지의 새로운 경로가 생성.\n","  2x2x2 건너뛰니까 \n","\n","   다시 말해 모델이 다양한 관점에서 블록 계산을 수행\n","  - 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과가 있음\n","\n","- 레이어 정규화\n","  - 미니 배치의 인스턴스( x )별로 평균을 빼주고 표준편차로 나눠줘 정규화(normalization)을 수행하는 기법\n","  - 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과\n","  \n","- 드롭아웃\n","  - 딥러닝 모델은 그 표현력이 아주 좋아서 학습 데이터 그 자체를 외워버릴 염려가 있습니다. 이를 과적합(overfitting)이라고 합니다. 드롭아웃(dropout)은 이러한 과적합 현상을 방지하고자 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외하는 기법\n","  - torch.nn.Dropout 객체는 뉴런별로 드롭아웃을 수행할지 말지를 확률적으로 결정하는 함수인데 p=0.2라는 말은 드롭아웃 수행 비율이 평균적으로 20%가 되게끔 하는 것임\n","\n","- 옵티마이저\n","  - 딥러닝 모델 학습은 모델 출력과 정답 사이의 오차(error)를 최소화하는 방향을 구하고 이 방향에 맞춰 모델 전체의 파라미터(parameter)들을 업데이트하는 과정. 이때 오차를 최소화하는 방향을 그래디언트(gradient)라고 하며 오차를 최소화하는 과정을 최적화(optimization)라고 함\n","  - 오차를 구하려면 현재 시점의 모델에 입력을 넣어봐서 처음부터 끝까지 계산해보고 정답과 비교해야 하며 오차를 구하기 위해 이같이 모델 처음부터 끝까지 순서대로 계산해보는 과정을 순전파(forward propagation)이라고 한다.\n","  - 오차를 구했다면 오차를 최소화하는 최초의 그래디언트를 구할 수 있으며 이는 미분(devative)으로 구합니다. 이후 미분의 연쇄 법칙(chain rule)에 따라 모델 각 가중치별 그래디언트 역시 구할 수 있다. 이 과정은 순전파의 역순으로 순차적으로 수행되는데 이를 역전파(backpropagation)라고 함\n","- 학습 과정은 미니 배치 단위로 이루어지며 이는 눈을 가린 상태에서 산등성이를 한걸음씩 내려가는 과정에 비유할 수 있다. 내가 지금 있는 위치에서 360도 모든 방향에 대해 한발한발 내딛어보고 가장 경사가 급한 쪽으로 한걸음씩 내려가는 과정을 반복하는 것이다.\n","  - 모델을 업데이트할 때(산등성이를 내려갈 때) 중요한 것은 방향과 보폭인데 이는 최적화 도구(optimizer)의 도움을 받는다. 트랜스포머 모델이 쓰는 최적화 도구가 바로 아담 옵티마이저(Adam Optimizer)이며 아담 옵티마이저는 오차를 줄이는 성능이 좋아서 트랜스포머 말고도 널리 쓰인다.\n","  - 아담 옵티마이저의 핵심 동작 원리는 방향과 보폭을 적절하게 정해주는 것이다. 방향을 정할 때는 현재 위치에서 가장 경사가 급한 쪽으로 내려가되, 여태까지 내려오던 관성(방향)을 일부 유지하도록 한다. 보폭의 경우 안가본 곳은 성큼 빠르게 걸어 훑고 많이 가본 곳은 갈수록 보폭을 줄여 세밀하게 탐색하는 방식으로 정한다.\n"],"metadata":{"id":"Jf_t2eVNE-9j"}},{"cell_type":"code","source":["import torch\n","x = torch.tensor([2,1])\n","w1 = torch.tensor([[3,2,-4],[2,-3,1]])\n","b1 = 1\n","w2 = torch.tensor([[-1, 1], [1,2], [3,1]])\n","b2 = -1"],"metadata":{"id":"y0U33hhvJqtr","executionInfo":{"status":"ok","timestamp":1673227846596,"user_tz":-540,"elapsed":8,"user":{"displayName":"정재훈","userId":"01263813800591133648"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["h_preact = torch.matmul(x, w1) + b1\n","h = torch.nn.functional.relu(h_preact)\n","y = torch.matmul(h, w2) + b2"],"metadata":{"id":"VC5GfqVfJr75","executionInfo":{"status":"ok","timestamp":1673227861485,"user_tz":-540,"elapsed":5,"user":{"displayName":"정재훈","userId":"01263813800591133648"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# 트랜스 포머 인코더 들어오는 것 잘이해하도록 압축 . 디코더는 타겟의 목적에 맞게끔 해준다. \n","# 트랜스 포머 셀프 에텐션은 문맥을 반영한다.\n","# 디코더 부분에서 쓰는게 버트 인코더 부분에서 쓰는게 gpt이다.\n"],"metadata":{"id":"g1mmF4h6Jvk3"},"execution_count":null,"outputs":[]}]}